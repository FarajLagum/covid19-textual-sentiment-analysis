{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21943da",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pyLDAvis.gensim\n",
    "import gensim\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\"In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d9304",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = './results/'\n",
    "data_dir = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from get_csv_files import get_csv_files\n",
    "\n",
    "# folders_and_files = get_csv_files(data_dir)\n",
    "\n",
    "# print(folders_and_files.keys())\n",
    "\n",
    "# for val in folders_and_files.values():\n",
    "#     for f in val:\n",
    "#         print(f,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# demo_file_name = demo_data_dir + \"05_Week4_May_2021_COVID19_Ottawa.csv\"\n",
    "demo_file_name = './data/months/01_January_2021_COVID19_Ottawa.csv'\n",
    "\n",
    "file_names = glob.glob(data_dir + \"/*/*.csv\")\n",
    "# print(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_data_df(demo_file_name):\n",
    "    documents_df = pd.read_csv(demo_file_name, on_bad_lines='skip')\n",
    "# documents_df['text_clean_textual'] = documents_df['text_clean_textual'].astype(str)\n",
    "# print(documents_df.head(3))\n",
    "# print(documents_df['Article'])\n",
    "\n",
    "    return documents_df\n",
    "\n",
    "\n",
    "documents_df = get_data_df(demo_file_name)\n",
    "documents_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# text processing\n",
    "# def initial_clean(text):\n",
    "#     text = re.sub(\"[^a-zA-Z ]\", \"\", text)\n",
    "#     text = text.lower()\n",
    "#     text = nltk.word_tokenize(text)\n",
    "#     return text\n",
    "\n",
    "def initial_clean(text):\n",
    "    text = str(text)\n",
    "    if isinstance(text, str) and text.strip():\n",
    "        text = re.sub(\"[^a-zA-Z ]\", \"\", text)\n",
    "        text = text.lower()\n",
    "    text = nltk.word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['say', 'use', 'not', 'would', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'took', 'per', 'cent', 'done', 'try', 'many', 'some', 'see', 'rather',\n",
    "                   'lot', 'lack', 'make', 'want', 'seem', 'even', 'also', 'may', 'take',\n",
    "                   'come', 'new', 'said', 'like', 'de'])\n",
    "\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def stem_words(text):\n",
    "    try:\n",
    "        text = [stemmer.stem(word) for word in text]\n",
    "        # no single letter words\n",
    "        text = [word for word in text if len(word) > 1]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "\n",
    "def pre_processing(text):\n",
    "    return stem_words(remove_stop_words(initial_clean(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# clean documents and create new column \"tokenized\"\n",
    "t1 = time.time()\n",
    "documents_df['tokenized_documents'] = documents_df['text_clean_textual'].apply(\n",
    "    pre_processing)\n",
    "t2 = time.time()\n",
    "# Time to clean and tokenize 3209 documents: 0.21254388093948365 min\n",
    "print(\"Time to clean and tokenize\", len(\n",
    "    documents_df), \"documents:\", (t2-t1)/60, \"min\")\n",
    "\n",
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# documents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized = documents_df['tokenized_documents']\n",
    "\n",
    "# Create a Gensim dictionary from the tokenized corpus\n",
    "dictionary = corpora.Dictionary(tokenized)\n",
    "\n",
    "# Filter out terms that occur in less than 5 document and more than 80% of the documents.\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.8)\n",
    "\n",
    "# Greate a bag of words\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in tokenized]\n",
    "\n",
    "# print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create LDA model\n",
    "\n",
    "NUM_TOPICS = 5\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(\n",
    "    corpus, num_topics=NUM_TOPICS, id2word=dictionary, passes=15, update_every=2,  iterations=5)\n",
    "\n",
    "# Save model\n",
    "lda_model.save('model_combined.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "get_document_topics = lda_model.get_document_topics(corpus[1])\n",
    "\n",
    "print(get_document_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topic in lda_model.show_topics(num_topics=NUM_TOPICS, num_words=10, formatted=False):\n",
    "    print('Topic {}:'.format(i))\n",
    "    for word, prob in topic:\n",
    "        print('\\t{}'.format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# visualizing topics\n",
    "lda_viz = gensim.models.ldamodel.LdaModel.load('model_combined.gensim')\n",
    "\n",
    "lda_display = pyLDAvis.gensim.prepare(\n",
    "    lda_viz, corpus, dictionary, sort_topics=True)\n",
    "\n",
    "\n",
    "# replace\n",
    "demo_file_name = demo_file_name.replace(\"data\", \"results\")\n",
    "\n",
    "\n",
    "pyLDAvis.save_html(lda_display, demo_file_name + '.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abdf602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one function\n",
    "\n",
    "\n",
    "def topic_modelling(file_name, num_topics=5):\n",
    "\n",
    "    documents_df = get_data_df(file_name)\n",
    "\n",
    "    documents_df['tokenized_documents'] = documents_df['text_clean_textual'].apply(\n",
    "        pre_processing)\n",
    "\n",
    "    documents_df.dropna(inplace=True)\n",
    "\n",
    "    tokenized = documents_df['tokenized_documents']\n",
    "\n",
    "    # Create a Gensim dictionary from the tokenized corpus\n",
    "    dictionary = corpora.Dictionary(tokenized)\n",
    "    # Filter out terms that appear in less than 5 document and more than 80% of the documents.\n",
    "    dictionary.filter_extremes(no_below=2, no_above=0.8)\n",
    "    # convert the dictionary to a bag of words corpus\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in tokenized]\n",
    "\n",
    "    # Create LDA model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(\n",
    "        corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "\n",
    "    # change the directory name to save the results in the results dir\n",
    "    file_name = file_name.replace(\"data\", \"results\")\n",
    "    file_name = file_name.replace(\".csv\", \"_\")\n",
    "\n",
    "    # Save model\n",
    "    lda_model.save(file_name + 'model_combined.gensim')\n",
    "\n",
    "    # visualizing topics\n",
    "    lda_viz = gensim.models.ldamodel.LdaModel.load(\n",
    "        file_name + 'model_combined.gensim')\n",
    "\n",
    "    lda_display = pyLDAvis.gensim.prepare(\n",
    "        lda_viz, corpus, dictionary, sort_topics=True)\n",
    "\n",
    "    # Save visualizations\n",
    "    pyLDAvis.save_html(lda_display, file_name + '.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02b3a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# apply to all files\n",
    "NUM_TOPICS = 5\n",
    "\n",
    "\n",
    "for file_name in file_names:\n",
    "    print(file_name)\n",
    "\n",
    "    topic_modelling(file_name, NUM_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e438ae30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "ta-sa",
   "language": "python",
   "name": "ta-sa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
